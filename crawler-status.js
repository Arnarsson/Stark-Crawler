import { createClient } from '@supabase/supabase-js';
import dotenv from 'dotenv';

dotenv.config();

const supabase = createClient(
  process.env.SUPABASE_URL,
  process.env.SUPABASE_KEY
);

console.log('üöÄ STARK Crawler Status Report');
console.log('================================\n');

// Check products
const { data: products, count: productCount } = await supabase
  .from('stark_products')
  .select('*', { count: 'exact' })
  .order('last_seen_at', { ascending: false })
  .limit(10);

console.log(`üì¶ Products in Database: ${productCount || 0}`);

if (products && products.length > 0) {
  console.log('\nüìã Latest Products Added:');
  products.forEach(p => {
    console.log(`   ‚Ä¢ ${p.name || 'No name'}`);
    console.log(`     SKU: ${p.sku || 'N/A'} | Price: ${p.price_numeric ? p.price_numeric + ' DKK' : 'N/A'}`);
  });
}

// Check crawl logs if table exists
const { data: logs } = await supabase
  .from('stark_crawl_logs')
  .select('*')
  .order('started_at', { ascending: false })
  .limit(1);

if (logs && logs.length > 0) {
  const log = logs[0];
  console.log('\nüìä Last Crawl Session:');
  console.log(`   Started: ${log.started_at}`);
  console.log(`   Status: ${log.status}`);
  if (log.products_added) console.log(`   Products Added: ${log.products_added}`);
  if (log.products_updated) console.log(`   Products Updated: ${log.products_updated}`);
  if (log.errors) console.log(`   Errors: ${log.errors}`);
}

console.log('\n‚úÖ System Status: OPERATIONAL');
console.log('   Supabase API: Connected');
console.log('   Database: Available');
console.log('\nüîß Configuration:');
console.log(`   Supabase URL: ${process.env.SUPABASE_URL}`);
console.log(`   Environment: Production`);

console.log('\nüìù Available Commands:');
console.log('   npm run test      - Test single product extraction');
console.log('   npm run crawl     - Run full crawler');
console.log('   npm run export    - Export to CSV');

console.log('\n================================');
console.log('Ready for n8n integration! üéâ');